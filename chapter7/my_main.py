# my_main.py
from dotenv import load_dotenv
from my_llm import MyLLM # 注意：这里导入我们自己的类
from hello_agents import HelloAgentsLLM

# 加载环境变量
load_dotenv()

# 实例化我们重写的客户端，并指定provider

llm = MyLLM(
    provider="ollama",
    model="qwen2:1.5b", # 需与 `ollama run` 指定的模型一致
    base_url="http://127.0.0.1:11434/v1",
    api_key="ollama" # 本地服务同样不需要真实 Key
)

# 准备消息
messages = [{"role": "user", "content": "你好，请介绍一下你自己。"}]

# 发起调用，think等方法都已从父类继承，无需重写
response_stream = llm.think(messages)

# 打印响应
print("ModelScope Response:")
for chunk in response_stream:
    # chunk在my_llm库中已经打印过一遍，这里只需要pass即可
    # print(chunk, end="", flush=True)
    pass